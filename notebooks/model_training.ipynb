{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "def create_directories():\n",
    "    directories = [\n",
    "        '../processed_dataset/real',\n",
    "        '../processed_dataset/fake',\n",
    "        '../models',\n",
    "        '../models/checkpoints',\n",
    "        '../logs',\n",
    "        '../logs/training_history',\n",
    "        '../evaluation'\n",
    "    ]\n",
    "    for dir in directories:\n",
    "        os.makedirs(dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameExtractor:\n",
    "    def __init__(self, sample_rate=30):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        )\n",
    "\n",
    "    def extract_faces_from_frame(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "\n",
    "        processed_faces = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y + h, x:x + w]\n",
    "            face = cv2.resize(face, (128, 128))\n",
    "            processed_faces.append(face)\n",
    "        return processed_faces\n",
    "\n",
    "    def process_video(self, video_path, output_dir):\n",
    "        \"\"\"Extract frames from a video and save them as images\"\"\"\n",
    "        try:\n",
    "            clip = VideoFileClip(video_path)\n",
    "            for i, frame in enumerate(clip.iter_frames()):\n",
    "                if i % self.sample_rate == 0:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    faces = self.extract_faces_from_frame(frame)\n",
    "                    if faces:\n",
    "                        for j, face in enumerate(faces):\n",
    "                            face_path = os.path.join(output_dir, f\"{os.path.basename(video_path)}_{i}_{j}.jpg\")\n",
    "                            cv2.imwrite(face_path, face)\n",
    "            clip.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(real_dir='../dataset/real_videos', fake_dir='../dataset/fake_videos'):\n",
    "    \"\"\"Prepare dataset and save frames to disk\"\"\"\n",
    "    extractor = VideoFrameExtractor(sample_rate=30)\n",
    "    # Process real videos\n",
    "    print(\"Processing real videos...\")\n",
    "    for video in os.listdir(real_dir):\n",
    "        video_path = os.path.join(real_dir, video)\n",
    "        output_dir = '../processed_dataset/real'\n",
    "        extractor.process_video(video_path, output_dir)\n",
    "\n",
    "    # Process fake videos\n",
    "    print(\"\\nProcessing fake videos...\")\n",
    "    for video in os.listdir(fake_dir):\n",
    "        video_path = os.path.join(fake_dir, video)\n",
    "        output_dir = '../processed_dataset/fake'\n",
    "        extractor.process_video(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset preparation...\n",
      "Processing real videos...\n",
      "\n",
      "Processing fake videos...\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "print(\"Starting dataset preparation...\")\n",
    "prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess images in batches\n",
    "def data_generator(batch_size, file_list, label):\n",
    "    \"\"\"Generate data batches from specific file lists.\"\"\"\n",
    "    num_samples = len(file_list)\n",
    "    while True:  # Infinite loop for generator\n",
    "        np.random.shuffle(file_list)\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_files = file_list[i:i + batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for file in batch_files:\n",
    "                file_path = file  # Direct file path\n",
    "                img = cv2.imread(file_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (224, 224))  # Resize images\n",
    "                    img = img / 255.0  # Normalize\n",
    "                    batch_images.append(img)\n",
    "                    batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data generators\n",
    "# Wrap Python generators with tf.data.Dataset\n",
    "batch_size = 32\n",
    "\n",
    "# Split dataset into training and validation\n",
    "all_real_files = [os.path.join('../processed_dataset/real', f) for f in os.listdir('../processed_dataset/real')]\n",
    "all_fake_files = [os.path.join('../processed_dataset/fake', f) for f in os.listdir('../processed_dataset/fake')]\n",
    "real_train, real_val = train_test_split(all_real_files, test_size=0.2, random_state=42)\n",
    "fake_train, fake_val = train_test_split(all_fake_files, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets\n",
    "real_train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(batch_size, real_train, 0),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "fake_train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(batch_size, fake_train, 1),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and shuffle training datasets\n",
    "train_dataset = tf.data.Dataset.zip((real_train_dataset, fake_train_dataset)).shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation datasets\n",
    "real_val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(batch_size, real_val, 0),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "fake_val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(batch_size, fake_val, 1),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine validation datasets\n",
    "val_generator = tf.data.Dataset.zip((real_val_dataset, fake_val_dataset)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create the model architecture\"\"\"\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(128, 128, 3)\n",
    "    )\n",
    "\n",
    "    # Freeze the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model = create_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f'../models/checkpoints/model_{timestamp}_{{epoch:02d}}-{{val_accuracy:.4f}}.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, None, 224, 224, 3), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (None, None, 224, 224, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(None, None), dtype=int32)')\n  • training=True\n  • mask=('None', 'None')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased epochs for better training\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreal_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreal_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\projects\\DeepfakeDetectionML\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\projects\\DeepfakeDetectionML\\env\\Lib\\site-packages\\keras\\src\\models\\functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, None, 224, 224, 3), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (None, None, 224, 224, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(None, None), dtype=int32)')\n  • training=True\n  • mask=('None', 'None')"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    train_dataset ,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,  # Increased epochs for better training\n",
    "    steps_per_epoch=len(real_train) // batch_size,\n",
    "    validation_steps=len(real_val) // batch_size,\n",
    "    callbacks=[model_checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and training history\n",
    "model.save(f'../models/deepfake_detector_{timestamp}.keras')\n",
    "with open(f'../logs/training_history/history_{timestamp}.json', 'w') as f:\n",
    "    json.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_true = np.concatenate([\n",
    "    np.zeros(len(real_val)),\n",
    "    np.ones(len(fake_val))\n",
    "])\n",
    "y_pred = model.predict(val_generator).flatten()\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "precision = precision_score(y_true, np.round(y_pred))\n",
    "recall = recall_score(y_true, np.round(y_pred))\n",
    "f1 = f1_score(y_true, np.round(y_pred))\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, np.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'auc': auc,\n",
    "    'confusion_matrix': conf_matrix.tolist()  # Convert to list for JSON serialization\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save confusion matrix image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.matshow(conf_matrix, fignum=1)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.colorbar()\n",
    "plt.savefig(f'../evaluation/confusion_matrix_{timestamp}.png')\n",
    "\n",
    "with open(f'../evaluation/results_{timestamp}.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f)\n",
    "\n",
    "print(\"Training and evaluation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
